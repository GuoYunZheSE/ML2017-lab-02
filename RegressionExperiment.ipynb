{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "def Draw(title,loops,train_loss,validation_loss,train_accuracy,val_accuracy,test_accuracy):\n",
    "    #the loss\n",
    "    plt.plot(np.arange(0,loops-1,1), train_loss[0:loops-1], label='Train Loss')\n",
    "    plt.plot(np.arange(0,loops-1,1), validation_loss[0:loops-1], label='Validation Loss')\n",
    "    plt.xlabel('loops')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('{} Loss'.format(title))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    #the accuracy\n",
    "    plt.plot(np.arange(0,loops-1,1), train_accuracy[0:loops-1], label='Train Accuracy')\n",
    "    plt.plot(np.arange(0,loops-1,1), val_accuracy[0:loops-1], label='Validation Accuracy')\n",
    "    plt.plot(np.arange(0, loops - 1, 1), test_accuracy[0:loops - 1], label='Test Accuracy')\n",
    "    plt.xlabel('loops')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('{} Accuracy'.format(title))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def DataChange(filepath):\n",
    "    # To change the -1 values in Dataset to 0\n",
    "    date=''\n",
    "    with open(filepath,'r+') as fr:\n",
    "        for eachline in fr.readlines():\n",
    "            if '-1' in eachline:\n",
    "                eachline=eachline.replace('-1','+0')\n",
    "            date=date+eachline\n",
    "    with open(filepath,'r+') as fw:\n",
    "        fw.writelines(date)\n",
    "\n",
    "\n",
    "def Max(M1,M2): # Return the Matrix with a bigger norm\n",
    "    N1=np.linalg.norm(M1)\n",
    "    N2=np.linalg.norm(M2)\n",
    "    if N1>=N2:\n",
    "        return N1\n",
    "    else:\n",
    "        return N2\n",
    "\n",
    "\n",
    "def sigmoid(X):\n",
    "    sig=1/(1+np.exp(-X))\n",
    "    return sig\n",
    "\n",
    "def hypothesis(W,X):\n",
    "    return sigmoid(X.dot(W))\n",
    "\n",
    "\n",
    "def loss(X,Y,W):\n",
    "    n=X.shape[0]\n",
    "    temp=Y.transpose().dot(np.log(hypothesis(W,X)))+(1-Y.transpose()).dot(np.log(1-hypothesis(W,X)))\n",
    "    loss=(-1*temp)/n\n",
    "    return loss\n",
    "\n",
    "\n",
    "def accuracy(X,Y,W,threshold):\n",
    "    m,n=X.shape\n",
    "    p = np.zeros(shape=(m, 1))\n",
    "    h=hypothesis(W,X)\n",
    "    hit=0\n",
    "    for it in range(0, h.shape[0]):\n",
    "        if h[it] >= threshold:\n",
    "            p[it, 0] = 1\n",
    "        else:\n",
    "            p[it, 0] = 0\n",
    "        if p[it,0]==Y[it][0]:\n",
    "            hit=hit+1\n",
    "    return hit/m\n",
    "\n",
    "\n",
    "def SGD(Parameters):\n",
    "    tic=time.time()\n",
    "    train_X=Parameters['Train_X']\n",
    "    train_Y=Parameters['Train_Y']\n",
    "    val_X=Parameters['Val_X']\n",
    "    val_Y=Parameters['Val_Y']\n",
    "    W=Parameters['Weights']\n",
    "    N=Parameters['Learning_Rate']\n",
    "    max_loop = Parameters['Max_Loops']\n",
    "    epsilon = Parameters['Epsilon']\n",
    "    threshold=Parameters['threshold']\n",
    "    Test_X=Parameters['Test_X']\n",
    "    Test_Y=Parameters['Test_Y']\n",
    "    count = 0\n",
    "    error = np.zeros((col, 1))\n",
    "    TL=[]#train loss\n",
    "    VL=[]#validation loss\n",
    "    train_accuracy=[]\n",
    "    val_accuracy=[]\n",
    "    test_accutacy=[]\n",
    "    while count <= max_loop:\n",
    "        i = random.randint(0, train_X.shape[0] - 1)\n",
    "        grad = (hypothesis(W, train_X[i]) - train_Y[i]) * train_X[i]\n",
    "        grad = grad.reshape(train_X.shape[1], 1)\n",
    "        count = count + 1\n",
    "        W = W - N * grad\n",
    "        if np.linalg.norm(W - error) < epsilon:\n",
    "            break\n",
    "        else:\n",
    "            error = W\n",
    "            Loss_Train = loss(train_X, train_Y, W)\n",
    "            Loss_Validation = loss(val_X, val_Y, W)\n",
    "            TL.append(Loss_Train[0])\n",
    "            VL.append(Loss_Validation[0])\n",
    "            train_accuracy.append(accuracy(train_X, train_Y, W, threshold))\n",
    "            val_accuracy.append(accuracy(val_X, val_Y, W, threshold))\n",
    "            test_accutacy.append(accuracy(Test_X, Test_Y, W, threshold))\n",
    "            print('Loop {}'.format(count), 'Loss_Train: ', Loss_Train, 'Loss_Validation: ',\n",
    "                    Loss_Validation)\n",
    "            print('Accuracy: Train: {}, Validation: {}, Test: {}'.format(train_accuracy[count - 1],\n",
    "                                                                             val_accuracy[count - 1],\n",
    "                                                                             test_accutacy[count - 1]))\n",
    "    print('SGD Completed. Time Used:{}'.format(time.time()-tic))\n",
    "    Draw('SGD',count,TL,VL,train_accuracy,val_accuracy,test_accutacy)\n",
    "    return VL,test_accutacy\n",
    "\n",
    "def Momentum(Parameters):\n",
    "    tic=time.time()\n",
    "    train_X=Parameters['Train_X']\n",
    "    train_Y=Parameters['Train_Y']\n",
    "    val_X=Parameters['Val_X']\n",
    "    val_Y=Parameters['Val_Y']\n",
    "    W=Parameters['Weights']\n",
    "    N=Parameters['Learning_Rate']\n",
    "    max_loop = Parameters['Max_Loops']\n",
    "    epsilon = Parameters['Epsilon']\n",
    "    threshold=Parameters['threshold']\n",
    "    Test_X=Parameters['Test_X']\n",
    "    Test_Y=Parameters['Test_Y']\n",
    "    gamma=Parameters['decoy_rate']\n",
    "    count = 0\n",
    "    error = np.zeros((train_X.shape[1], 1))\n",
    "    velocity=np.zeros((train_X.shape[1],1))\n",
    "    TL=[]#train loss\n",
    "    VL=[]#validation loss\n",
    "    train_accuracy=[]\n",
    "    val_accuracy=[]\n",
    "    test_accutacy=[]\n",
    "    while count <= max_loop:\n",
    "        count = count + 1\n",
    "        i=random.randint(0,train_X.shape[0]-1)\n",
    "\n",
    "        # Compute grad\n",
    "        grad=(hypothesis(W,train_X[i])-train_Y[i])*train_X[i]\n",
    "        grad=grad.reshape(train_X.shape[1],1)\n",
    "        # Compute velocity\n",
    "        velocity=gamma*velocity+N*grad\n",
    "        # Gradient decent\n",
    "        W=W-velocity\n",
    "\n",
    "        if np.linalg.norm(W - error) < epsilon:\n",
    "            break\n",
    "        else:\n",
    "            error = W\n",
    "            Loss_Train = loss(train_X,train_Y,W)\n",
    "            Loss_Validation = loss(val_X,val_Y,W)\n",
    "            TL.append(Loss_Train[0])\n",
    "            VL.append(Loss_Validation[0])\n",
    "            train_accuracy.append(accuracy(train_X,train_Y,W,threshold))\n",
    "            val_accuracy.append(accuracy(val_X,val_Y,W,threshold))\n",
    "            test_accutacy.append(accuracy(Test_X,Test_Y,W,threshold))\n",
    "            print('Loop {}'.format(count), 'Loss_Train: ', Loss_Train, 'Loss_Validation: ',\n",
    "                  Loss_Validation)\n",
    "            print('Accuracy: Train: {}, Validation: {}, Test: {}'.format(train_accuracy[count-1],val_accuracy[count-1],test_accutacy[count-1]))\n",
    "    Draw('Momentum',count,TL,VL,train_accuracy,val_accuracy,test_accutacy)\n",
    "    print('Momentum Completed Successfully. Time used:{:.2f}'.format(time.time()-tic))\n",
    "    return VL,test_accutacy\n",
    "\n",
    "def NAG(Parameters):\n",
    "    tic=time.time()\n",
    "    train_X=Parameters['Train_X']\n",
    "    train_Y=Parameters['Train_Y']\n",
    "    val_X=Parameters['Val_X']\n",
    "    val_Y=Parameters['Val_Y']\n",
    "    W=Parameters['Weights']\n",
    "    N=Parameters['Learning_Rate']\n",
    "    max_loop = Parameters['Max_Loops']\n",
    "    epsilon = Parameters['Epsilon']\n",
    "    threshold=Parameters['threshold']\n",
    "    Test_X=Parameters['Test_X']\n",
    "    Test_Y=Parameters['Test_Y']\n",
    "    gamma=Parameters['decoy_rate']\n",
    "    count = 0\n",
    "    error = np.zeros((train_X.shape[1], 1))\n",
    "    velocity=np.zeros((train_X.shape[1],1))\n",
    "    velocity_prev = np.zeros((train_X.shape[1], 1))\n",
    "    TL=[]#train loss\n",
    "    VL=[]#validation loss\n",
    "    train_accuracy=[]\n",
    "    val_accuracy=[]\n",
    "    test_accutacy=[]\n",
    "    while count <= max_loop:\n",
    "        count = count + 1\n",
    "        i=random.randint(0,train_X.shape[0]-1)\n",
    "        # Compute grad\n",
    "        grad=(hypothesis(W,train_X[i])-train_Y[i])*train_X[i]\n",
    "        grad=grad.reshape(train_X.shape[1],1)\n",
    "        # Compute velocity\n",
    "        velocity_prev=velocity\n",
    "        velocity=gamma*velocity-N*grad\n",
    "        # Gradient decent\n",
    "        W=W-gamma*velocity_prev+(1+gamma)*velocity\n",
    "\n",
    "        # Use relative error to decide whether to stop\n",
    "        if np.linalg.norm(W - error)/Max(W,error) < epsilon:\n",
    "            break\n",
    "        else:\n",
    "            error = W\n",
    "            Loss_Train = loss(train_X,train_Y,W)\n",
    "            Loss_Validation = loss(val_X,val_Y,W)\n",
    "            TL.append(Loss_Train[0])\n",
    "            VL.append(Loss_Validation[0])\n",
    "            train_accuracy.append(accuracy(train_X,train_Y,W,threshold))\n",
    "            val_accuracy.append(accuracy(val_X,val_Y,W,threshold))\n",
    "            test_accutacy.append(accuracy(Test_X,Test_Y,W,threshold))\n",
    "            print('Loop {}'.format(count), 'Loss_Train: ', Loss_Train, 'Loss_Validation: ',\n",
    "                  Loss_Validation)\n",
    "            print('Accuracy: Train: {}, Validation: {}, Test: {}'.format(train_accuracy[count-1],val_accuracy[count-1],test_accutacy[count-1]))\n",
    "    Draw('NAG',count,TL,VL,train_accuracy,val_accuracy,test_accutacy)\n",
    "    print('Momentum Completed Successfully. Time used:{:.2f}'.format(time.time()-tic))\n",
    "    return VL, test_accutacy\n",
    "\n",
    "def Adagrad(Parameters):\n",
    "    # This method is NGA with the adaptive learning rate method: Adagrad\n",
    "    tic=time.time()\n",
    "    train_X=Parameters['Train_X']\n",
    "    train_Y=Parameters['Train_Y']\n",
    "    val_X=Parameters['Val_X']\n",
    "    val_Y=Parameters['Val_Y']\n",
    "    W=Parameters['Weights']\n",
    "    N=Parameters['Learning_Rate']\n",
    "    max_loop = Parameters['Max_Loops']\n",
    "    epsilon = Parameters['Epsilon']\n",
    "    threshold=Parameters['threshold']\n",
    "    Test_X=Parameters['Test_X']\n",
    "    Test_Y=Parameters['Test_Y']\n",
    "\n",
    "    # Initialize\n",
    "    count = 0\n",
    "    error = np.zeros((train_X.shape[1], 1))\n",
    "    cache=np.zeros((train_X.shape[1],1))\n",
    "    eps=0.000001\n",
    "    TL=[]#train loss\n",
    "    VL=[]#validation loss\n",
    "    train_accuracy=[]\n",
    "    val_accuracy=[]\n",
    "    test_accutacy=[]\n",
    "\n",
    "    while count <= max_loop:\n",
    "        count = count + 1\n",
    "        i=random.randint(0,train_X.shape[0]-1)\n",
    "        # Compute grad\n",
    "        grad=(hypothesis(W,train_X[i])-train_Y[i])*train_X[i]\n",
    "        grad=grad.reshape(train_X.shape[1],1)\n",
    "        # Compute Cache\n",
    "        cache=cache+(grad**2)\n",
    "        # Gradient decent\n",
    "        W=W-N*grad/(np.sqrt(cache)+eps)\n",
    "\n",
    "        # Use relative error to decide whether to stop\n",
    "        if np.linalg.norm(W - error)/Max(W,error) < epsilon:\n",
    "            break\n",
    "        else:\n",
    "            error = W\n",
    "            Loss_Train = loss(train_X,train_Y,W)\n",
    "            Loss_Validation = loss(val_X,val_Y,W)\n",
    "            TL.append(Loss_Train[0])\n",
    "            VL.append(Loss_Validation[0])\n",
    "            train_accuracy.append(accuracy(train_X,train_Y,W,threshold))\n",
    "            val_accuracy.append(accuracy(val_X,val_Y,W,threshold))\n",
    "            test_accutacy.append(accuracy(Test_X,Test_Y,W,threshold))\n",
    "            print('Loop {}'.format(count), 'Loss_Train: ', Loss_Train, 'Loss_Validation: ',\n",
    "                  Loss_Validation)\n",
    "            print('Accuracy: Train: {}, Validation: {}, Test: {}'.format(train_accuracy[count-1],val_accuracy[count-1],test_accutacy[count-1]))\n",
    "    Draw('Adagrad',count,TL,VL,train_accuracy,val_accuracy,test_accutacy)\n",
    "    print('Adagrad Completed Successfully. Time used:{:.2f}'.format(time.time()-tic))\n",
    "    return VL, test_accutacy\n",
    "\n",
    "def AdaDelta(Parameters):\n",
    "    # Load parameters from Parameters\n",
    "    tic = time.time()\n",
    "    train_X = Parameters['Train_X']\n",
    "    train_Y = Parameters['Train_Y']\n",
    "    val_X = Parameters['Val_X']\n",
    "    val_Y = Parameters['Val_Y']\n",
    "    W = Parameters['Weights']\n",
    "    max_loop = Parameters['Max_Loops']\n",
    "    epsilon = Parameters['Epsilon']\n",
    "    threshold = Parameters['threshold']\n",
    "    Test_X = Parameters['Test_X']\n",
    "    Test_Y = Parameters['Test_Y']\n",
    "    decay_rate=Parameters['decoy_rate']\n",
    "\n",
    "    # Initialize\n",
    "    count = 0\n",
    "    error = np.zeros((train_X.shape[1], 1))\n",
    "    cache = np.zeros((train_X.shape[1], 1))\n",
    "    delt  = np.zeros((train_X.shape[1], 1))\n",
    "    eps = 0.000001\n",
    "    TL = []  # train loss\n",
    "    VL = []  # validation loss\n",
    "    train_accuracy = []\n",
    "    val_accuracy = []\n",
    "    test_accutacy = []\n",
    "\n",
    "    while count <= max_loop:\n",
    "        count = count + 1\n",
    "        i = random.randint(0, train_X.shape[0] - 1)\n",
    "\n",
    "        # Compute grad\n",
    "        grad = (hypothesis(W, train_X[i]) - train_Y[i]) * train_X[i]\n",
    "        grad = grad.reshape(train_X.shape[1], 1)\n",
    "\n",
    "        # Compute Cache\n",
    "        cache = decay_rate*cache +(1-decay_rate)*(grad ** 2)\n",
    "\n",
    "        # Gradient decent\n",
    "        deltW = -np.sqrt(delt+eps)*grad / (np.sqrt(cache) + eps)\n",
    "        W=W+deltW\n",
    "        delt  =decay_rate*delt+(1-decay_rate)*deltW**2\n",
    "        # Use relative error to decide whether to stop\n",
    "        if np.linalg.norm(W - error) / Max(W, error) < epsilon:\n",
    "            break\n",
    "        else:\n",
    "            error = W\n",
    "            Loss_Train = loss(train_X, train_Y, W)\n",
    "            Loss_Validation = loss(val_X, val_Y, W)\n",
    "            TL.append(Loss_Train[0])\n",
    "            VL.append(Loss_Validation[0])\n",
    "            train_accuracy.append(accuracy(train_X, train_Y, W, threshold))\n",
    "            val_accuracy.append(accuracy(val_X, val_Y, W, threshold))\n",
    "            test_accutacy.append(accuracy(Test_X, Test_Y, W, threshold))\n",
    "            print('Loop {}'.format(count), 'Loss_Train: ', Loss_Train, 'Loss_Validation: ',\n",
    "                  Loss_Validation)\n",
    "            print('Accuracy: Train: {}, Validation: {}, Test: {}'.format(train_accuracy[count - 1],\n",
    "                                                                         val_accuracy[count - 1],\n",
    "                                                                         test_accutacy[count - 1]))\n",
    "    Draw('AdaDelta',count, TL, VL, train_accuracy, val_accuracy, test_accutacy)\n",
    "    print('AdaDelta Completed Successfully. Time used:{:.2f}'.format(time.time() - tic))\n",
    "    return VL, test_accutacy\n",
    "\n",
    "def RMSprop(Parameters):\n",
    "    # This method is NGA with the adaptive learning rate method: Adagrad\n",
    "    tic=time.time()\n",
    "    train_X=Parameters['Train_X']\n",
    "    train_Y=Parameters['Train_Y']\n",
    "    val_X=Parameters['Val_X']\n",
    "    val_Y=Parameters['Val_Y']\n",
    "    W=Parameters['Weights']\n",
    "    N=Parameters['Learning_Rate']\n",
    "    max_loop = Parameters['Max_Loops']\n",
    "    epsilon = Parameters['Epsilon']\n",
    "    threshold=Parameters['threshold']\n",
    "    Test_X=Parameters['Test_X']\n",
    "    Test_Y=Parameters['Test_Y']\n",
    "    decay_rate=Parameters['decoy_rate']\n",
    "\n",
    "    # Initialize\n",
    "    count = 0\n",
    "    error = np.zeros((train_X.shape[1], 1))\n",
    "    cache=np.zeros((train_X.shape[1],1))\n",
    "    eps=0.000001\n",
    "    TL=[]#train loss\n",
    "    VL=[]#validation loss\n",
    "    train_accuracy=[]\n",
    "    val_accuracy=[]\n",
    "    test_accutacy=[]\n",
    "\n",
    "    while count <= max_loop:\n",
    "        count = count + 1\n",
    "        i=random.randint(0,train_X.shape[0]-1)\n",
    "        # Compute grad\n",
    "        grad=(hypothesis(W,train_X[i])-train_Y[i])*train_X[i]\n",
    "        grad=grad.reshape(train_X.shape[1],1)\n",
    "        # Compute Cache\n",
    "        cache=decay_rate*cache+(1-decay_rate)*(grad**2)\n",
    "        # Gradient decent\n",
    "        W=W-N*grad/(np.sqrt(cache)+eps)\n",
    "\n",
    "        # Use relative error to decide whether to stop\n",
    "        if np.linalg.norm(W - error)/Max(W,error) < epsilon:\n",
    "            break\n",
    "        else:\n",
    "            error = W\n",
    "            Loss_Train = loss(train_X,train_Y,W)\n",
    "            Loss_Validation = loss(val_X,val_Y,W)\n",
    "            TL.append(Loss_Train[0])\n",
    "            VL.append(Loss_Validation[0])\n",
    "            train_accuracy.append(accuracy(train_X,train_Y,W,threshold))\n",
    "            val_accuracy.append(accuracy(val_X,val_Y,W,threshold))\n",
    "            test_accutacy.append(accuracy(Test_X,Test_Y,W,threshold))\n",
    "            print('Loop {}'.format(count), 'Loss_Train: ', Loss_Train, 'Loss_Validation: ',\n",
    "                  Loss_Validation)\n",
    "            print('Accuracy: Train: {}, Validation: {}, Test: {}'.format(train_accuracy[count-1],val_accuracy[count-1],test_accutacy[count-1]))\n",
    "    Draw('RMSprop',count,TL,VL,train_accuracy,val_accuracy,test_accutacy)\n",
    "    print('RMSprop Completed Successfully. Time used:{:.2f}'.format(time.time()-tic))\n",
    "    return VL, test_accutacy\n",
    "\n",
    "def Adam(Parameters):\n",
    "    # This method is NGA with the adaptive learning rate method: Adagrad\n",
    "    tic=time.time()\n",
    "    train_X=Parameters['Train_X']\n",
    "    train_Y=Parameters['Train_Y']\n",
    "    val_X=Parameters['Val_X']\n",
    "    val_Y=Parameters['Val_Y']\n",
    "    W=Parameters['Weights']\n",
    "    N=Parameters['Learning_Rate']\n",
    "    max_loop = Parameters['Max_Loops']\n",
    "    epsilon = Parameters['Epsilon']\n",
    "    threshold=Parameters['threshold']\n",
    "    Test_X=Parameters['Test_X']\n",
    "    Test_Y=Parameters['Test_Y']\n",
    "    eps=Parameters['eps']\n",
    "    beta1=Parameters['Beta1']\n",
    "    beta2 = Parameters['Beta2']\n",
    "\n",
    "    # Initialize\n",
    "    count = 0\n",
    "    error = np.zeros((train_X.shape[1], 1))\n",
    "    m = np.zeros((train_X.shape[1],1))\n",
    "    v = np.zeros((train_X.shape[1], 1))\n",
    "    TL=[]#train loss\n",
    "    VL=[]#validation loss\n",
    "    train_accuracy=[]\n",
    "    val_accuracy=[]\n",
    "    test_accutacy=[]\n",
    "\n",
    "    while count <= max_loop:\n",
    "        count = count + 1\n",
    "        i=random.randint(0,train_X.shape[0]-1)\n",
    "\n",
    "        # Compute grad\n",
    "        grad=(hypothesis(W,train_X[i])-train_Y[i])*train_X[i]\n",
    "        grad=grad.reshape(train_X.shape[1],1)\n",
    "\n",
    "        # Compute\n",
    "        m=beta1*m+(1-beta1)*grad\n",
    "        mt=m/(1-beta1**count)\n",
    "        v=beta2*v+(1-beta2)*(grad**2)\n",
    "        vt=v/(1-beta2**count)\n",
    "        # Gradient decent\n",
    "        W=W-N*mt/(np.sqrt(vt)+eps)\n",
    "\n",
    "        # Use relative error to decide whether to stop\n",
    "        if np.linalg.norm(W - error)/Max(W,error) < epsilon:\n",
    "            break\n",
    "        else:\n",
    "            error = W\n",
    "            Loss_Train = loss(train_X,train_Y,W)\n",
    "            Loss_Validation = loss(val_X,val_Y,W)\n",
    "            TL.append(Loss_Train[0])\n",
    "            VL.append(Loss_Validation[0])\n",
    "            train_accuracy.append(accuracy(train_X,train_Y,W,threshold))\n",
    "            val_accuracy.append(accuracy(val_X,val_Y,W,threshold))\n",
    "            test_accutacy.append(accuracy(Test_X,Test_Y,W,threshold))\n",
    "            print('Loop {}'.format(count), 'Loss_Train: ', Loss_Train, 'Loss_Validation: ',\n",
    "                  Loss_Validation)\n",
    "            print('Accuracy: Train: {}, Validation: {}, Test: {}'.format(train_accuracy[count-1],val_accuracy[count-1],test_accutacy[count-1]))\n",
    "    Draw('Adam',count,TL,VL,train_accuracy,val_accuracy,test_accutacy)\n",
    "    print('Adam Completed Successfully. Time used:{:.2f}'.format(time.time()-tic))\n",
    "    return VL, test_accutacy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Read Data\n",
    "    tic=time.time()\n",
    "    Data_Path = '/home/lucas/Codes/GitHub/ML_Assignment1/ML_Assignment1/DataSet/a9a.txt'\n",
    "    Test_Path='/home/lucas/Codes/GitHub/ML_Assignment1/ML_Assignment1/DataSet/a9a.t'\n",
    "    Data_Parameter, Data_Value = load_svmlight_file(Data_Path)\n",
    "    Test_Parameter,Test_Value=load_svmlight_file(Test_Path)\n",
    "    Test_Parameter=Test_Parameter.toarray()\n",
    "    Test_Parameter=np.hstack([Test_Parameter,np.zeros(shape=(Test_Parameter.shape[0],1))])\n",
    "    Test_Value=Test_Value.reshape(Test_Value.shape[0],1)\n",
    "    Data_Parameter = Data_Parameter.toarray()\n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(Data_Parameter, Data_Value, test_size=0.3, random_state=1)\n",
    "    t_row = train_X.shape[0]  # Row Size\n",
    "    col = train_X.shape[1]  # Column Size\n",
    "    v_row = val_X.shape[0]\n",
    "    train_Y = train_Y.reshape(t_row, 1)\n",
    "    val_Y = val_Y.reshape(v_row, 1)\n",
    "    W = np.random.random(size=(col, 1))\n",
    "    Parameter={'Train_X':train_X,\n",
    "               'Train_Y':train_Y,\n",
    "               'Val_X':val_X,\n",
    "               'Val_Y':val_Y,\n",
    "               'Test_X':Test_Parameter,\n",
    "               'Test_Y':Test_Value,\n",
    "               'Weights':W,\n",
    "               'Learning_Rate':0.01,\n",
    "               'Max_Loops':5000,\n",
    "               'Epsilon':0.00000001,\n",
    "               'threshold':0.4,\n",
    "               'decoy_rate':0.9,\n",
    "               'eps':0.00000001,\n",
    "               'Beta1':0.9,\n",
    "               'Beta2':0.999\n",
    "               }\n",
    "    SGD_VL,SGD_test_accuracy=SGD(Parameter)\n",
    "    Momentum_VL,Momentum_test_accuracy=Momentum(Parameter)\n",
    "    NAG_VL,NAG_test_accuracy=NAG(Parameter)\n",
    "    Adagrad_VL,Adagrad_test_accuracy=Adagrad(Parameter)\n",
    "    AdaDelta_VL,AdaDelta_test_accuracy=AdaDelta(Parameter)\n",
    "    Adam_VL,Adam_test_accuracy=Adam(Parameter)\n",
    "    print('All Time Used:{:0.2f}s'.format(time.time()-tic))\n",
    "\n",
    "    plt.plot(np.arange(0,Parameter['Max_Loops']-1,1), SGD_VL[0:Parameter['Max_Loops']-1], label='SGD')\n",
    "    plt.plot(np.arange(0,Parameter['Max_Loops']-1,1), Momentum_VL[0:Parameter['Max_Loops']-1], label='Momentum')\n",
    "    plt.plot(np.arange(0, Parameter['Max_Loops'] - 1, 1), NAG_VL[0:Parameter['Max_Loops'] - 1], label='NAG')\n",
    "    plt.plot(np.arange(0, Parameter['Max_Loops'] - 1, 1), Adagrad_VL[0:Parameter['Max_Loops'] - 1], label='Adagrad')\n",
    "    plt.plot(np.arange(0, Parameter['Max_Loops'] - 1, 1), AdaDelta_VL[0:Parameter['Max_Loops'] - 1], label='AdaDelta')\n",
    "    plt.plot(np.arange(0, Parameter['Max_Loops'] - 1, 1), Adam_VL[0:Parameter['Max_Loops'] - 1], label='Adam')\n",
    "    plt.xlabel('loops')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.plot(np.arange(0,Parameter['Max_Loops']-1,1), SGD_test_accuracy[0:Parameter['Max_Loops']-1], label='SGD')\n",
    "    plt.plot(np.arange(0,Parameter['Max_Loops']-1,1), Momentum_test_accuracy[0:Parameter['Max_Loops']-1], label='Momentum')\n",
    "    plt.plot(np.arange(0, Parameter['Max_Loops'] - 1, 1), NAG_test_accuracy[0:Parameter['Max_Loops'] - 1], label='NAG')\n",
    "    plt.plot(np.arange(0, Parameter['Max_Loops'] - 1, 1), Adagrad_test_accuracy[0:Parameter['Max_Loops'] - 1], label='Adagrad')\n",
    "    plt.plot(np.arange(0, Parameter['Max_Loops'] - 1, 1), AdaDelta_test_accuracy[0:Parameter['Max_Loops'] - 1], label='AdaDelta')\n",
    "    plt.plot(np.arange(0, Parameter['Max_Loops'] - 1, 1), Adam_test_accuracy[0:Parameter['Max_Loops'] - 1], label='Adam')\n",
    "    plt.xlabel('loops')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
